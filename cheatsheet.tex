\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage{amsmath}
\usepackage[landscape]{geometry}

% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex latexsheet.tex
%
% 2.
%  latex latexsheet.tex
%  dvips -P pdf  -t landscape latexsheet.dvi
%  ps2pdf latexsheet.ps


% If you're reading this, be prepared for confusion.  Making this was
% a learning experience for me, and it shows.  Much of the placement
% was hacked in; if you make it better, let me know...


% 2008-04
% Changed page margin code to use the geometry package. Also added code for
% conditional page margins, depending on paper size. Thanks to Uwe Ziegenhagen
% for the suggestions.

% 2006-08
% Made changes based on suggestions from Gene Cooperman. <gene at ccs.neu.edu>


% To Do:
% \listoffigures \listoftables
% \setcounter{secnumdepth}{0}


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\textbf{Machine-Learning Cheat Sheet}} \\
\end{center}

\section{Estimating Parameters}

\subsection{Maximum a Posteriori (MAP)}

\begin{equation*}
	\underset{\theta}{\argmax} \ P(D | \theta)
\end{equation*}

\subsection{Maximum Likelihood Estimation (MLE)}

\begin{equation*}
	\underset{\theta}{\argmax} \ P(\theta | D)
	= \underset{\theta}{\argmax} \ \frac{P(D | \theta) P(\theta)}{P(D)}
	= \underset{\theta}{\argmax} \ P(D | \theta) P(\theta)
\end{equation*}

\section{Regression}

\subsection{Linear Regression}

Find optimal $w$ and $b$ such that the average distance of points from the line is minimized:

\begin{equation*}
\underset{w,b}{\argmax} \ \sum_{i=1}^{n} \frac{1}{2} \left( w x_i + b - y_i \right)^2
\end{equation*}


\section{Classification}

\subsection{Logistic Regression}

\subsection{Naive Bayes}

Assumes that features are independent of one another. Thus, learning and classification for each feature happens independently of other features.

\emph{Classifies} input vector $<x_1, ..., x_n>$ as class $y$ according to
\begin{align*}
	\underset{y}{\argmax} \ P(y | x_1, ..., x_n)
	&= \underset{y}{\argmax} \ P(x_1, ..., x_n | y) P(y)\\
	&= \underset{y}{\argmax} \ P(y) \prod_{i}^{n} P(x_i | y)
\end{align*}

\emph{Learns} $P(x_i | y)$ for
\begin{itemize}
	\item \emph{Discrete $x_i$} -- $P(x_i | y) = \frac{\#D(X_i = x_i, Y = y)}{\#D(Y = y)}$
	
	For smoothing, use $P(x_i | y) = \frac{\#D(X_i = x_i, Y = y) + k}{\#D(Y = y) + n_i k}$, where $n_i$ is the number of different possible values for $X_i$
	\item \emph{Continuous $x_i$} -- Can use any PDF, but usually use Gaussian $P(x_i | y) = \mathcal{N}(\mu_{X_i | y}, \sigma_{X_i | y}^2)$, where $\mu_{X_i | y}$ and $\sigma_{X_i | y}$ are, respectively, the average and variance of $X_i$ for all data points where $Y = y$. The Gaussian distribution already provides smoothing.
\end{itemize}

\subsection{Perceptron}

\subsection{SVM}

\end{multicols}
\end{document}
