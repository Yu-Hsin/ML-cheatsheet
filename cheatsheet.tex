\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage{amsmath}
\usepackage[landscape]{geometry}

% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex latexsheet.tex
%
% 2.
%  latex latexsheet.tex
%  dvips -P pdf  -t landscape latexsheet.dvi
%  ps2pdf latexsheet.ps


% If you're reading this, be prepared for confusion.  Making this was
% a learning experience for me, and it shows.  Much of the placement
% was hacked in; if you make it better, let me know...


% 2008-04
% Changed page margin code to use the geometry package. Also added code for
% conditional page margins, depending on paper size. Thanks to Uwe Ziegenhagen
% for the suggestions.

% 2006-08
% Made changes based on suggestions from Gene Cooperman. <gene at ccs.neu.edu>


% To Do:
% \listoffigures \listoftables
% \setcounter{secnumdepth}{0}


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\textbf{Machine Learning Cheat Sheet}} \\
\end{center}

\section{Estimating Parameters}

\subsection{Maximum a Posteriori (MAP)}

\begin{equation*}
	\underset{\theta}{\argmax} \ P(D | \theta)
\end{equation*}

\subsection{Maximum Likelihood Estimation (MLE)}

\begin{equation*}
	\underset{\theta}{\argmax} \ P(\theta | D)
	= \underset{\theta}{\argmax} \ \frac{P(D | \theta) P(\theta)}{P(D)}
	= \underset{\theta}{\argmax} \ P(D | \theta) P(\theta)
\end{equation*}

\section{Regression}

\subsection{Linear Regression}

Find optimal $w$ and $b$ such that the average distance of points from the line is minimized:
\begin{equation*}
\underset{w,b}{\argmax} \ \sum_{i=1}^{n} \frac{1}{2} \left( w x_i + b - y_i \right)^2
\end{equation*}
Differentiate, and solution is
\begin{equation*}
\left[ \sum_{i = 1}^{n} x_i x_i^T \right] w = \sum_{i = 1}^{n} y_i x_i
\end{equation*}

Eigenvalues of $x^T x$ are positive. Use $x^T M x \geq 0 \:\forall x \in R^n$ to prove this:
\begin{align*}
v^T \sum_{i} x_i x_i^T v \geq 0\\
\sum_{i} (v^t x_i)^2 \geq 0
\end{align*}

\emph{Note} that when the data does not span the entire space, $x^T x$ does not have full rank, and thus is non-invertible. This can be fixed by either decomposition through removing eigenvalues below a certain threshold, or adding a small $\epsilon$ to the diagonal of the matrix.

\section{Classification}

\subsection{Logistic Regression}

\subsection{Naive Bayes}

Assumes that features are independent of one another given the class. Thus, learning and classification for each feature happens independently of other features.

\emph{Classifies} input vector $<x_1, ..., x_n>$ as class $y$ according to
\begin{align*}
	\underset{y}{\argmax} \ P(y | x_1, ..., x_n)
	&= \underset{y}{\argmax} \ P(x_1, ..., x_n | y) P(y)\\
	&= \underset{y}{\argmax} \ P(y) \prod_{i}^{n} P(x_i | y)
\end{align*}

\emph{Learns} $P(x_i | y)$ for
\begin{itemize}
	\item \emph{Discrete $x_i$} -- $P(x_i | y) = \frac{\#D(X_i = x_i, Y = y)}{\#D(Y = y)}$
	
	For smoothing, use $P(x_i | y) = \frac{\#D(X_i = x_i, Y = y) + k}{\#D(Y = y) + n_i k}$, where $n_i$ is the number of different possible values for $X_i$
	\item \emph{Continuous $x_i$} -- Can use any PDF, but usually use Gaussian $P(x_i | y) = \mathcal{N}(\mu_{X_i | y}, \sigma_{X_i | y}^2)$, where $\mu_{X_i | y}$ and $\sigma_{X_i | y}$ are, respectively, the average and variance of $X_i$ for all data points where $Y = y$. The Gaussian distribution already provides smoothing.
\end{itemize}

\subsection{Perceptron}

Produces linear decision boundaries.

\emph{Classifies} using $\hat{y} = X_{test}\:w +  b$

\emph{Learns} $w$ and $b$ by updating $w$ whenever $y_i (w^T x_i + b) \leq 0$ (i.e. incorrectly classified). Updates as such:
\begin{align*}
w &\leftarrow w + x_i y_i\\
b &\leftarrow b + y_i
\end{align*}
Repeat until all examples are correctly classified.

$w$ is some linear combination $\sum_{i} \alpha_i x_i$ of data points, and decision boundary is the linear hyperplane $f(x) = w^T x + b$.

Note that the perceptron is the same as stochastic gradient descent with a hinge loss function of $max(0, 1 - y_i [<w, x_i> + b])$

\subsection{SVM}

\section{Convergence of...}

\subsection{Gradient Descent}

\subsection{Stochastic Gradient Descent}

\subsection{Newton Method}

\subsection{Perceptron}

Assume
\begin{itemize}
	\item $\exists w^*$ such that $||w^*|| = 1$
	\item $\exists \gamma > 0$ such that $\forall t = 1 ... n,\ y_t(x_t \cdot w^*) \geq \gamma$
	\item $\forall t = 1 ... n,\ ||x_t|| \leq R$
\end{itemize}

Then the perceptron algorithm makes at most $R^2 / \gamma^2$ errors.

Proof
\begin{itemize}
	\item Bound $w^{k + 1} \cdot w^* \rightarrow ||w^{k + 1}|| \geq k \gamma$
	\item Bound $||w^{k + 1}||^2
	$\begin{align*}
		k R^2 &\geq ||w^{k + 1}||^2 \geq k^2 \gamma^2\\
		\frac{R^2}{\gamma^2} &\geq k
	\end{align*}
\end{itemize}
If we don't include $b$ as part of $w$, this becomes
\begin{equation*}
	k \leq \frac{(b^* + 1) (r^2 + 1)}{\gamma^2}
\end{equation*}

\subsection{SVM}

\section{Optimization}

\subsection{Linear Programming}

\subsection{Quadratic Programming}

\subsection{Gradient Descent}

Guess $x_0$ to be a local minimum of $F(x)$, then obtain $x_{n + 1} = x_n - \gamma_n \nabla F(x_n)$, where $n \geq 0$.

If $F$ is convex, then this converges to the global solution.

\subsection{Stochastic Gradient Descent}

Essentially the same as gradient descent, except that at each iteration only one training point is taken into consideration instead of the entire training set.

Randomly shuffle data points, and then repeat for $i = 1 ... n$, $\theta_{t + 1} := \theta_t - \alpha \delta_\theta (y_i, <\phi(x_i), \theta_t>)$ until convergence, where $\delta_\theta(y_i, <\phi(x_i), \theta>) = \frac{\delta}{\delta \theta} \left[ \frac{1}{2} (y_i - <x_i, w>)^2 \right]$ (essentially taking the derivative of the loss function).

Since it is a randomized algorithm, it doesn't give the same values every time. Run $n$ times and pick the best. It reaches close to the optimum, but not exactly.
\begin{equation*}
\textmd{Regret} = \textmd{Current Performance} - \textmd{Best possible performance} < \ensuremath{\mathcal{O}}(1/\sqrt{t})
\end{equation*}
$\alpha$ is the learning rate.

\subsection{Newton's Method}

Choose an initial point that is close enough to $x^*$, then update
\begin{equation*}
x_{new} = x_{old} - \frac{f'(x)}{f''(x)}
\end{equation*}
Converges quadratically in the region near the optimal value, given that the Hessian can be found and that $f(x)$ is twice differentiable. Inside the bound, convergence is
\begin{equation*}
||x_{n + 1} - x^*|| \leq \gamma || \left[ \delta_x^2 f(x_n) \right]^{-1} || ||x_n - x^*||^2
\end{equation*}
Outside the bound, convergence is
\begin{equation*}
|| \delta_x f(x^*) - \delta_x f(x) - <x^* - x, \delta_x^2 f(x)> || \leq \gamma ||x^* - x||^2
\end{equation*}

BFGS approximates Newton's method. It computes the Hessian by approximating the gradient, so computation of inverse is not required.

\section{Parzen Window}

\begin{align*}
p_{emp}(x) &= \frac{1}{n} \sum_{i = 1}^{n} \delta_{xi}(x)\\
\hat{p}(x) &= \frac{1}{n} \sum_{i = 1}^{n} k_{xi}(x)
\end{align*}

Conditions for a smoothing kernel are $\int k(x) dx = 1$, $\int x k(x) dx = 0$, and $\int x^2 k(x) dx = 0$.

\section{Duality / KKT}

If the Karush-Kuhn-Tucker conditions
\begin{itemize}
	\item \emph{Primal constraint satisfaction:} Satisfies original constraints
	\item \emph{Dual constraint satisfaction:} $\delta_x L(x^*, \alpha^*) \leq 0$
	\item \emph{Complementary slackness:} $\lambda_i f_i(x) = 0,\:i = 1, ..., m$
	\item \emph{Lagrangian gradient condition:} $\nabla_x f_0(x) + \sum_{i = 1}^{m} \lambda_i \nabla_x f_i(x) + \sum_{i = 1}^{p} v_i \nabla_x h_i(x) = 0$
\end{itemize}

\section{Math}

Given a vector-valued function $F: R^n \rightarrow R^m$ (i.e. it is given by $m$ real-valued component functions of the form $F_i(x_1, ..., x_n)$), its Jacobian is:
\begin{equation*}
J_F(x_1, ..., x_n) = \frac{\delta (F_1, ..., F_m)}{\delta (x_1, ..., x_n)} = \left[ \begin{array}{ccc}
	\frac{\delta F_1}{\delta x_1} & ... & \frac{\delta F_1}{\delta x_n}\\
	\vdots & \ddots & \vdots\\
	\frac{\delta F_n}{\delta x_1} & ... & \frac{\delta F_n}{\delta x_n}
 \end{array} \right]
\end{equation*}

Given a real-valued function $f(x_1, x_2, ..., x_n)$, the second partial derivatives of which all exist and are continuous over the domain of $f$, its Hessian is:
\begin{equation*}
H(f) = \left[ \begin{array}{cccc}
	\frac{\delta^2 f}{\delta x_1^2} & \frac{\delta^2 f}{\delta x_1\,\delta x_2} & ... & \frac{\delta^2 f}{\delta x_1\,\delta x_n}\\
	\frac{\delta^2 f}{\delta x_2\,\delta x_1} & \frac{\delta^2 f}{\delta x_2^2} & ... & \frac{\delta^2 f}{\delta x_2\,\delta x_n}\\
	\vdots & \vdots & \ddots & \vdots\\
	\frac{\delta^2 f}{\delta x_n\,\delta x_1} & \frac{\delta^2 f}{\delta x_n\,\delta x_2} & ... & \frac{\delta^2 f}{\delta x_n^2}
 \end{array} \right]
\end{equation*}
This is related to the Jacobian by $H(f)(x) = J(\nabla f)(x)$

\subsection{Convexity}

\subsubsection{Convexity}

Set $C$ is convex if $\lambda x + (1 - \lambda) y \in C\;\forall x,y \in C,\:\lambda \in [0,1]$

Function $f$ is convex if its epigraph is a convex set; i.e. all these three conditions are equivalent:
\begin{align*}
f(\lambda x + (1 - \lambda) y) &\leq \lambda f(x) + (1 - \lambda) f(y), \; \lambda \in [0,1]\\
f(y) - f(x) &\geq f'(x) (y - x), \; x,y \in \textmd{dom }f\\
f''(x) &\geq 0
\end{align*}

If $f(x)$ and $g(x)$ are convex, then $a f(x)$, $f(Ax + b)$, $f(x) + g(x)$, $max(f(x), g(x))$, and $g(f(x))$ are also convex.

\subsubsection{Strict convexity}

Has unique minima. Here the Hessian condition is sufficient but \emph{not} necessary for strict convexity.
\begin{align*}
f(\lambda x + (1 - \lambda) y) &< \lambda f(x) + (1 - \lambda) f(y), \; \lambda \in [0,1]\\
f(y) - f(x) &> f'(x) (y - x), \; x,y \in \textmd{dom }f\\
f''(x) &> 0
\end{align*}

\subsubsection{Strong convexity}

If $m$ is the curvature coefficient, then
\begin{align*}
f(\lambda x + (1 - \lambda) y) &\leq \lambda f(x) + (1 - \lambda) f(y) - \frac{m}{2} \lambda (1 - \lambda) ||x - y||_2^2\\
f(y) - f(x) &\geq f'(x) (y - x) + \frac{m}{2} ||y - x||_2^2\\
f''(x) &\geq mI
\end{align*}

\subsubsection{Convex Hull}

Given a convex set $C$, the convex hull is $\{\bar{x} | \bar{x} = \sum_{i = 1}^{n} \alpha_i x_i,\;x_i \in C,\}$ for any values of $\alpha_i$ satisfying $\sum_{i = 1}^{n} \alpha_i \leq 1$.

$\underset{x}{sup} f(x) = \underset{x \in Co_f}{sup} f(x)$ where $Co_f$ is the convex hull of $f$.

\end{multicols}
\end{document}
