\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage{amsmath}
\usepackage[landscape]{geometry}

% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex latexsheet.tex
%
% 2.
%  latex latexsheet.tex
%  dvips -P pdf  -t landscape latexsheet.dvi
%  ps2pdf latexsheet.ps


% If you're reading this, be prepared for confusion.  Making this was
% a learning experience for me, and it shows.  Much of the placement
% was hacked in; if you make it better, let me know...


% 2008-04
% Changed page margin code to use the geometry package. Also added code for
% conditional page margins, depending on paper size. Thanks to Uwe Ziegenhagen
% for the suggestions.

% 2006-08
% Made changes based on suggestions from Gene Cooperman. <gene at ccs.neu.edu>


% To Do:
% \listoffigures \listoftables
% \setcounter{secnumdepth}{0}


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\textbf{Machine-Learning Cheat Sheet}} \\
\end{center}

\section{Estimating Parameters}

\subsection{Maximum a Posteriori (MAP)}

\begin{equation*}
	\underset{\theta}{\argmax} \ P(D | \theta)
\end{equation*}

\subsection{Maximum Likelihood Estimation (MLE)}

\begin{equation*}
	\underset{\theta}{\argmax} \ P(\theta | D)
	= \underset{\theta}{\argmax} \ \frac{P(D | \theta) P(\theta)}{P(D)}
	= \underset{\theta}{\argmax} \ P(D | \theta) P(\theta)
\end{equation*}

\section{Regression}

\subsection{Linear Regression}

Find optimal $w$ and $b$ such that the average distance of points from the line is minimized:

\begin{equation*}
\underset{w,b}{\argmax} \ \sum_{i=1}^{n} \frac{1}{2} \left( w x_i + b - y_i \right)^2
\end{equation*}


\section{Classification}

\subsection{Logistic Regression}

\subsection{Naive Bayes}

Assumes that features are independent of one another given the class. Thus, learning and classification for each feature happens independently of other features.

\emph{Classifies} input vector $<x_1, ..., x_n>$ as class $y$ according to
\begin{align*}
	\underset{y}{\argmax} \ P(y | x_1, ..., x_n)
	&= \underset{y}{\argmax} \ P(x_1, ..., x_n | y) P(y)\\
	&= \underset{y}{\argmax} \ P(y) \prod_{i}^{n} P(x_i | y)
\end{align*}

\emph{Learns} $P(x_i | y)$ for
\begin{itemize}
	\item \emph{Discrete $x_i$} -- $P(x_i | y) = \frac{\#D(X_i = x_i, Y = y)}{\#D(Y = y)}$
	
	For smoothing, use $P(x_i | y) = \frac{\#D(X_i = x_i, Y = y) + k}{\#D(Y = y) + n_i k}$, where $n_i$ is the number of different possible values for $X_i$
	\item \emph{Continuous $x_i$} -- Can use any PDF, but usually use Gaussian $P(x_i | y) = \mathcal{N}(\mu_{X_i | y}, \sigma_{X_i | y}^2)$, where $\mu_{X_i | y}$ and $\sigma_{X_i | y}$ are, respectively, the average and variance of $X_i$ for all data points where $Y = y$. The Gaussian distribution already provides smoothing.
\end{itemize}

\subsection{Perceptron}

Produces linear decision boundaries.

\emph{Classifies} using the non-linear sigmoid (logistic) decision function:
\begin{equation*}
\hat{y} = \frac{1}{1+e^{- (w^T x + b)}} - 0.5
\end{equation*}

\emph{Learns} $w$ and $b$ by updating $w$ whenever $\hat{y} y \leq 0$ (i.e. incorrectly classified). Updates as such:
\begin{align*}
w &\leftarrow w + x_i y_i\\
b &\leftarrow b + y_i
\end{align*}
Repeat until all examples are correctly classified.

$w$ is some linear combination $\sum_{i} \alpha_i x_i$ of data points, and decision boundary is the linear hyperplane $f(x) = w^T x + b$.

Note that the perceptron is the same as stochastic gradient descent with a hinge loss function of $max(0, 1 - y_i [<w, x_i> + b])$

\subsection{SVM}

\section{Convergence of...}

\subsection{Gradient Descent}

\subsection{Stochastic Gradient Descent}

\subsection{Newton Method}

\subsection{Perceptron}

Assume
\begin{itemize}
	\item $\exists w^*$ such that $||w^*|| = 1$
	\item $\exists \gamma > 0$ such that $\forall t = 1 ... n,\ y_t(x_t \cdot w^*) \geq \gamma$
	\item $\forall t = 1 ... n,\ ||x_t|| \leq R$
\end{itemize}

Then the perceptron algorithm makes at most $R^2 / \gamma^2$ errors.

Proof
\begin{itemize}
	\item Bound $w^{k + 1} \cdot w^* \rightarrow ||w^{k + 1}|| \geq k \gamma$
	\item Bound $||w^{k + 1}||^2
	$\begin{align*}
		k R^2 &\geq ||w^{k + 1}||^2 \geq k^2 \gamma^2\\
		\frac{R^2}{\gamma^2} &\geq k
	\end{align*}
\end{itemize}
If we don't include $b$ as part of $w$, this becomes
\begin{equation*}
	k \leq \frac{(b^* + 1) (r^2 + 1)}{\gamma^2}
\end{equation*}

\subsection{SVM}

\end{multicols}
\end{document}
